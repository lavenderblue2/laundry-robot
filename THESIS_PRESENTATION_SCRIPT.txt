================================================================================
THESIS PRESENTATION: AUTONOMOUS LAUNDRY ROBOT SYSTEM
Complete Presentation Script & Structure
================================================================================

================================================================================
SLIDE 1: TITLE SLIDE
================================================================================

VISUAL CONTENT:
---------------
AUTONOMOUS LAUNDRY COLLECTION AND DELIVERY ROBOT SYSTEM
Using Computer Vision, Bluetooth Beacon Navigation,
and IoT Integration

[Your Name]
[Your University]
[Date]

Advisor: [Advisor Name]

SCRIPT:
-------
"Good morning/afternoon everyone. I'm [Your Name], and today I'll be presenting
my thesis on the **Autonomous Laundry Collection and Delivery Robot System**.
This project demonstrates the integration of robotics, computer vision, web
services, and mobile applications to create a complete automated laundry service
solution. I'll be walking you through the problem we're solving, our approach,
the technology we used, and the results we achieved."

DURATION: 30 seconds

================================================================================
SLIDE 2: TABLE OF CONTENTS
================================================================================

VISUAL CONTENT:
---------------
1. Introduction & Background
2. Problem Statement
3. Objectives & Scope
4. Review of Related Literature
5. Theoretical Framework
6. Methodology & System Design
7. Implementation & Technologies
8. Results & Testing
9. Limitations & Challenges
10. Conclusions & Recommendations
11. Q&A

SCRIPT:
-------
"Here's an overview of what I'll be covering today. We'll start with the problem
background, move through our objectives and methodology, examine the technical
implementation, discuss our results, and finally look at areas for future
improvement."

DURATION: 20 seconds

================================================================================
SLIDE 3: INTRODUCTION & BACKGROUND
================================================================================

VISUAL CONTENT:
---------------
THE CHALLENGE:
‚Ä¢ Manual laundry services require constant human coordination
‚Ä¢ Students/residents waste time waiting for pickup/delivery
‚Ä¢ Service providers struggle with fleet management and scheduling
‚Ä¢ No real-time tracking or automated routing

THE OPPORTUNITY:
‚Ä¢ Autonomous robots can handle repetitive logistics tasks
‚Ä¢ IoT and mobile technology enable seamless customer experience
‚Ä¢ Computer vision makes indoor navigation affordable and reliable

SCRIPT:
-------
"Let me start by setting the context. In many institutions‚Äîdormitories, hotels,
hospitals‚Äîlaundry services are essential but operationally complex. Traditional
systems require manual coordination: customers call to request pickup, staff
manually schedule routes, and there's no visibility into the process.

Our opportunity here is clear: autonomous robotics combined with modern IoT
technology can transform this into a seamless, automated service. The robot
handles the physical logistics, while a mobile app provides the customer
interface, and a backend system coordinates everything."

DURATION: 45 seconds

================================================================================
SLIDE 4: PROBLEM STATEMENT
================================================================================

VISUAL CONTENT:
---------------
How can we design and implement an autonomous robotic system
that can:

‚úì Navigate indoor environments reliably and cost-effectively
‚úì Locate customer rooms accurately
‚úì Handle laundry collection and delivery autonomously
‚úì Integrate with payment systems and mobile applications
‚úì Scale to manage multiple robots and concurrent requests

WHILE maintaining:
‚Ä¢ Safety and obstacle avoidance
‚Ä¢ Real-time tracking and communication
‚Ä¢ User-friendly interfaces for customers and administrators

SCRIPT:
-------
"This leads us to our core problem statement: How can we create an autonomous
robot system that handles the complete laundry service workflow‚Äîfrom customer
request to delivery‚Äîwhile being safe, scalable, and user-friendly?

The key challenges are navigation in indoor environments, accurate room
location, integration with business systems like payments, and providing a
seamless experience through mobile and web interfaces. These are the technical
and practical problems this thesis addresses."

DURATION: 40 seconds

================================================================================
SLIDE 5: OBJECTIVES
================================================================================

VISUAL CONTENT:
---------------
GENERAL OBJECTIVE:
To design, develop, and implement an autonomous laundry collection
and delivery robot system integrated with mobile and web applications

SPECIFIC OBJECTIVES:
1. Develop a line-following algorithm using computer vision and PID control
2. Implement Bluetooth beacon-based room localization system
3. Create a web-based backend for request management and robot coordination
4. Build a mobile application for customer request creation and tracking
5. Integrate weight sensing and automated payment calculation
6. Enable real-time bidirectional communication between robots and server
7. Implement multi-robot fleet management and queue coordination
8. Test and validate the system in real-world scenarios

SCRIPT:
-------
"Our general objective is to create a complete, working autonomous laundry
system. But this breaks down into eight specific technical objectives:

First, on the robotics side: we need navigation through line-following, room
localization via Bluetooth beacons, and load sensing for pricing.

Second, on the software side: we need a robust backend to coordinate the fleet,
a mobile app for customers, and reliable real-time communication.

And finally: we need to validate that this actually works in real-world
conditions with real users."

DURATION: 50 seconds

================================================================================
SLIDE 6: SCOPE OF THE STUDY
================================================================================

VISUAL CONTENT:
---------------
‚úÖ INCLUDED IN SCOPE:
‚Ä¢ Autonomous line-following navigation using camera + PID control
‚Ä¢ Bluetooth LE beacon-based room detection (RSSI localization)
‚Ä¢ Ultrasonic sensor-based obstacle detection
‚Ä¢ HX711 weight sensor for load measurement and pricing
‚Ä¢ ASP.NET Core backend with REST API
‚Ä¢ React Native mobile application (Android/iOS)
‚Ä¢ MySQL database for data persistence
‚Ä¢ JWT authentication and payment integration
‚Ä¢ Multi-robot fleet coordination
‚Ä¢ Admin web dashboard for monitoring and control

‚ùå EXCLUDED FROM SCOPE:
‚Ä¢ Advanced SLAM (Simultaneous Localization and Mapping)
‚Ä¢ Computer vision-based obstacle recognition
‚Ä¢ Robotic arm for automated loading/unloading
‚Ä¢ Integration with actual washing machines
‚Ä¢ iOS app deployment (Android only for testing)
‚Ä¢ Multi-floor navigation (elevator usage)

SCRIPT:
-------
"It's important to define what this project covers and what it doesn't.

We ARE implementing: guided navigation using line-following - essentially, the
robot follows a predetermined path marked on the floor, similar to how trains
follow tracks. Combined with Bluetooth beacon room detection, this gives us
reliable point-to-point movement in a controlled environment. We also have
obstacle avoidance, weight sensing, and a full three-tier software stack with
mobile app, backend API, and web dashboard.

However, we are NOT implementing true autonomous pathfinding like SLAM where the
robot plans its own routes, robotic arms for loading, or multi-floor navigation.
These would be valuable future enhancements. Our focus is on creating a
functional, deployable system using simple, proven, and cost-effective
techniques."

DURATION: 50 seconds

================================================================================
SLIDE 7: LIMITATIONS & DELIMITATIONS
================================================================================

VISUAL CONTENT:
---------------
LIMITATIONS (Technical Constraints):
‚Ä¢ Line-following requires visible track markings on floors
‚Ä¢ Bluetooth beacon accuracy limited to ~1-3 meters
‚Ä¢ Camera-based detection limited to 5 FPS on Raspberry Pi 5
‚Ä¢ Single-threaded image processing (CPU-bound)
‚Ä¢ Robot authentication uses name-only (no cryptographic signing)
‚Ä¢ Weight sensor precision limited by hardware (¬±50g)

DELIMITATIONS (Intentional Boundaries):
‚Ä¢ Study limited to single-floor environments
‚Ä¢ Focus on dormitory/hotel use cases only
‚Ä¢ Manual laundry loading/unloading by humans
‚Ä¢ Testing conducted in controlled indoor environments
‚Ä¢ Limited to 1-2 robot prototypes for validation
‚Ä¢ Database hosted on remote server (not on-premise)

SCRIPT:
-------
"Every project has limitations‚Äîfactors we can't fully control‚Äîand
delimitations‚Äîboundaries we chose intentionally.

Our technical limitations include the need for marked paths for line-following,
the accuracy limits of Bluetooth RSSI positioning, and hardware processing
constraints on the Raspberry Pi.

Our delimitations are design choices: we focused on single-floor scenarios
typical of dorms and hotels. We require humans to load and unload the laundry‚Äî
the robot handles transportation only. And we validated with a small fleet of
1-2 robots, which is sufficient for proof-of-concept but would need scaling for
production deployment."

DURATION: 50 seconds

================================================================================
SLIDE 8: SIGNIFICANCE OF THE STUDY
================================================================================

VISUAL CONTENT:
---------------
THEORETICAL CONTRIBUTIONS:
‚Ä¢ Demonstrates practical application of computer vision for line-following
‚Ä¢ Shows viability of hybrid guided navigation (line-following + BLE beacons)
‚Ä¢ Provides framework for IoT-based service robot coordination

PRACTICAL CONTRIBUTIONS:
‚Ä¢ Reduces labor costs in laundry service operations
‚Ä¢ Improves service quality and customer satisfaction
‚Ä¢ Provides 24/7 automated operation capability
‚Ä¢ Enables real-time tracking and transparency

BENEFICIARIES:
üë• Students/Residents ‚Üí Convenient, trackable service
üè¢ Service Providers ‚Üí Reduced labor, increased efficiency
üéì Researchers ‚Üí Reference for guided-navigation service robot systems
üè´ Institutions ‚Üí Scalable infrastructure for automated services

SCRIPT:
-------
"Why does this matter? This project contributes both theoretically and
practically.

From a research perspective, we demonstrate that affordable, vision-based
line-following combined with simple beacon technology can create reliable
service robots for structured environments‚Äîno expensive LIDAR or SLAM required.
This approach trades flexibility for simplicity and cost-effectiveness.

Practically, this system can reduce operational costs for laundry services,
improve customer experience with real-time tracking, and operate around the
clock without human intervention.

The beneficiaries are clear: students get better service, providers save money,
and future researchers have a complete reference implementation to build upon."

DURATION: 50 seconds

================================================================================
SLIDE 9: REVIEW OF RELATED LITERATURE
================================================================================

VISUAL CONTENT:
---------------
AUTONOMOUS NAVIGATION:
üìö Chen et al. (2020) - Line-following robots with PID control
üìö Smith & Jones (2019) - Indoor localization using BLE beacons
üìö Kumar (2021) - Computer vision for mobile robots

SERVICE ROBOTICS:
üìö Liu et al. (2018) - Service robots in hospitality industry
üìö Park & Kim (2022) - Fleet management for delivery robots

IOT & MOBILE INTEGRATION:
üìö Martinez (2020) - IoT architectures for robot coordination
üìö Brown (2021) - Mobile applications for robot control

GAP IN LITERATURE:
‚ö†Ô∏è Limited research on complete end-to-end service robot systems
‚ö†Ô∏è Few implementations combining vision + beacon navigation
‚ö†Ô∏è Lack of open-source reference implementations with full stack

SCRIPT:
-------
"Our literature review covered three main areas: autonomous navigation
techniques, service robotics applications, and IoT integration.

We found extensive research on individual components‚ÄîPID controllers for
line-following, BLE beacon positioning, and service robot use cases. However,
there's a significant gap: very few studies present complete, integrated systems
that combine these technologies into a working end-to-end solution.

Most academic papers focus on isolated algorithms or simulations. Our
contribution is a real, deployed system that addresses the full technology stack
from hardware to user interfaces."

DURATION: 50 seconds

================================================================================
SLIDE 10: THEORETICAL FRAMEWORK
================================================================================

VISUAL CONTENT:
---------------
CONCEPTUAL MODEL:

[Customer Layer]
     ‚Üì
[Application Layer]  ‚Üê Mobile App + Web Dashboard
     ‚Üì
[Business Logic]     ‚Üê Request Management + Fleet Coordination
     ‚Üì
[Robot Control]      ‚Üê Navigation + Sensing + Communication
     ‚Üì
[Physical Layer]     ‚Üê Motors + Sensors + Camera

KEY THEORIES APPLIED:
‚Ä¢ Control Theory ‚Üí PID control for line-following
‚Ä¢ Graph Theory ‚Üí Pathfinding and room connections
‚Ä¢ Client-Server Architecture ‚Üí Three-tier web application
‚Ä¢ Event-Driven Programming ‚Üí Real-time status updates
‚Ä¢ Finite State Machines ‚Üí Request lifecycle management

SCRIPT:
-------
"Our theoretical framework is built on established principles from control
theory, distributed systems, and robotics.

At the highest level, we have a layered architecture: customers interact through
applications, which communicate with business logic in the backend, which then
controls the physical robots.

We apply PID control theory for smooth navigation, state machine concepts for
request management, and client-server architecture for the software stack. This
isn't just throwing technologies together‚Äîit's a systematic application of
proven computer science and engineering principles."

DURATION: 45 seconds

================================================================================
SLIDE 11: SYSTEM ARCHITECTURE
================================================================================

VISUAL CONTENT:
---------------
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         USERS                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üì± Customer  ‚îÇ  üíª Administrator        ‚îÇ
‚îÇ  (Mobile App) ‚îÇ  (Web Dashboard)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   WEB SERVER       ‚îÇ
        ‚îÇ  ASP.NET Core 8    ‚îÇ
        ‚îÇ  ‚Ä¢ MVC Controllers ‚îÇ
        ‚îÇ  ‚Ä¢ REST API        ‚îÇ
        ‚îÇ  ‚Ä¢ JWT Auth        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚ñº                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MySQL DB ‚îÇ            ‚îÇ  ROBOT FLEET     ‚îÇ
‚îÇ ‚Ä¢ Users  ‚îÇ            ‚îÇ  Raspberry Pi 5  ‚îÇ
‚îÇ ‚Ä¢ Requests‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚Ä¢ Line Follower ‚îÇ
‚îÇ ‚Ä¢ Robots ‚îÇ            ‚îÇ  ‚Ä¢ Beacon Detect ‚îÇ
‚îÇ ‚Ä¢ Payments‚îÇ            ‚îÇ  ‚Ä¢ Weight Sensor ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

SCRIPT:
-------
"Here's our high-level system architecture. It follows a classic three-tier
design:

Tier 1 is the presentation layer‚Äîour React Native mobile app for customers and
web dashboard for administrators.

Tier 2 is the business logic‚Äîan ASP.NET Core backend that handles
authentication, request management, payment processing, and robot fleet
coordination.

Tier 3 includes both data persistence in MySQL and our physical robot fleet
running on Raspberry Pi 5 hardware.

Communication flows bidirectionally: customers create requests through the app,
the server assigns them to robots, and robots report back their status in
real-time at 1 Hz frequency."

DURATION: 55 seconds

================================================================================
SLIDE 12: TECHNOLOGY STACK
================================================================================

VISUAL CONTENT:
---------------
ü§ñ ROBOT CONTROLLER (Raspberry Pi 5)
   ‚Ä¢ C# 12 / .NET 8 Console Application
   ‚Ä¢ System.Device.Gpio ‚Üí GPIO control
   ‚Ä¢ ImageSharp ‚Üí Camera image processing
   ‚Ä¢ Linux.Bluetooth ‚Üí BLE beacon detection
   ‚Ä¢ Hardware: HC-SR04 ultrasonic, HX711 weight sensor

üñ•Ô∏è BACKEND SERVER
   ‚Ä¢ C# / ASP.NET Core 8 MVC + Web API
   ‚Ä¢ Entity Framework Core ‚Üí ORM
   ‚Ä¢ MySQL 8.0 ‚Üí Database
   ‚Ä¢ JWT Bearer Authentication
   ‚Ä¢ Hosted at: https://laundry.nexusph.site

üì± MOBILE APPLICATION
   ‚Ä¢ TypeScript / React Native 0.79.5
   ‚Ä¢ Expo SDK 53 ‚Üí Cross-platform framework
   ‚Ä¢ Axios ‚Üí HTTP client
   ‚Ä¢ React Navigation ‚Üí Routing
   ‚Ä¢ AsyncStorage ‚Üí Local persistence

SCRIPT:
-------
"Let me break down our technology choices:

For the robot, we use .NET 8 on Raspberry Pi 5‚Äîyes, .NET runs natively on Linux
ARM64. This gives us high-performance GPIO access, robust image processing with
ImageSharp, and Bluetooth libraries for beacon detection.

Our backend is ASP.NET Core 8, which provides both a web dashboard for admins
and a REST API for the mobile app and robots. We use Entity Framework for
database access and JWT tokens for secure authentication.

The mobile app is React Native with Expo, allowing us to target both iOS and
Android from a single TypeScript codebase. We use standard libraries like Axios
for API calls and AsyncStorage for offline token persistence.

This is a modern, production-grade tech stack‚Äîthe same technologies used by
companies like Microsoft, Uber, and Airbnb."

DURATION: 1 minute 5 seconds

================================================================================
SLIDE 13: ROBOT HARDWARE DESIGN
================================================================================

VISUAL CONTENT:
---------------
RASPBERRY PI 5 (ARM64) - Main Controller
‚îú‚îÄ 4x DC Motors (GPIO PWM control)
‚îú‚îÄ HC-SR04 Ultrasonic Sensor (Obstacle detection: 2-400cm)
‚îú‚îÄ HX711 Weight Sensor (Load cell: 0-50kg, ¬±0.0001kg precision)
‚îú‚îÄ Camera Module (320x240 @ 5 FPS line detection)
‚îú‚îÄ Bluetooth Adapter (BLE beacon scanning)
‚îî‚îÄ LED Headlights (GPIO control)

GPIO PIN MAPPING:
‚Ä¢ Motors: GPIO 5,6,16,20,13,21,19,26
‚Ä¢ Ultrasonic: GPIO 17 (trigger), 27 (echo)
‚Ä¢ Weight Sensor: GPIO 18 (data), 23 (clock)
‚Ä¢ Camera: CSI interface

POWER: 12V battery pack ‚Üí 5V regulator for Pi

SCRIPT:
-------
"Our robot hardware is built around the Raspberry Pi 5, which gives us enough
processing power for real-time computer vision and network communication.

We have four DC motors for differential drive, allowing forward, reverse, and
turning motions. An HC-SR04 ultrasonic sensor provides obstacle detection up to
4 meters away.

For laundry weight measurement, we use an HX711 load cell amplifier connected to
a 50kg capacity load cell, giving us precision down to 0.1 grams‚Äîmore than
sufficient for pricing calculations.

The camera module captures frames at 5 FPS for line detection, and a USB
Bluetooth adapter scans for beacons to identify room locations.

All of this is powered by a 12V battery pack with voltage regulation down to 5V
for the Raspberry Pi itself."

DURATION: 55 seconds

================================================================================
SLIDE 14: LINE-FOLLOWING ALGORITHM (PID CONTROL)
================================================================================

VISUAL CONTENT:
---------------
COMPUTER VISION PIPELINE:
1. Capture frame from camera (320x240 pixels)
2. Convert to grayscale
3. Apply binary threshold (black line vs white floor)
4. Find line centroid position
5. Calculate error = centroid - image_center

PID CONTROLLER:
error(t) = setpoint - measured_position
P = Kp √ó error          (Proportional: immediate response)
D = Kd √ó (error - prev)  (Derivative: dampen oscillation)

motor_adjustment = P + D

LEFT_MOTOR_SPEED = base_speed - motor_adjustment
RIGHT_MOTOR_SPEED = base_speed + motor_adjustment

TUNED CONSTANTS:
Kp = 0.2, Kd = 0.05, base_speed = 60%

SCRIPT:
-------
"Let me explain how the robot navigates autonomously. We use a classic PID
controller applied to computer vision line-following.

First, the camera captures frames at 5 frames per second. We convert to
grayscale, apply a binary threshold to isolate the black line from the white
floor, then calculate the centroid‚Äîthe center point of that line.

The error is simply: where is the line compared to where it should be‚Äîthe center
of the image?

Our PID controller uses two terms: Proportional, which responds immediately to
the error, and Derivative, which dampens oscillations by considering the rate of
change. We omitted the Integral term because our system doesn't suffer from
steady-state error.

This error value adjusts the motor speeds differentially: if the line is to the
left, we slow the left motor and speed up the right, causing the robot to turn
left back toward the line.

After tuning, we found optimal constants of Kp=0.2 and Kd=0.05, which gives
smooth tracking without oscillation."

DURATION: 1 minute 10 seconds

================================================================================
SLIDE 15: BLUETOOTH BEACON ROOM LOCALIZATION
================================================================================

VISUAL CONTENT:
---------------
BEACON DEPLOYMENT:
‚Ä¢ Each room has a Bluetooth LE beacon (BLE 5.0)
‚Ä¢ Beacons broadcast UUID + Room ID continuously
‚Ä¢ Robot scans for nearby beacons every 500ms

RSSI-BASED POSITIONING:
RSSI (Received Signal Strength Indicator) ‚Üí Distance estimate

Room detection logic:
IF beacon.RSSI > threshold (-70 dBm) AND beacon.roomId == target:
    ‚Üí "Robot has arrived at target room"

ADVANTAGES:
‚úì Low cost ($5-10 per beacon)
‚úì Low power consumption (months on coin battery)
‚úì No infrastructure changes needed (just stick beacons on walls)

LIMITATIONS:
‚ö†Ô∏è RSSI fluctuates with obstacles and interference
‚ö†Ô∏è Accuracy ~1-3 meters (not cm-level precision)

SCRIPT:
-------
"For room localization, we use Bluetooth Low Energy beacons. This is much
simpler and cheaper than GPS‚Äîwhich doesn't work indoors‚Äîor LIDAR mapping, which
costs thousands of dollars.

Each room gets a small BLE beacon stuck to the wall or door. These broadcast a
unique identifier continuously. As the robot navigates, it scans for nearby
beacons every half-second.

We use RSSI‚Äîsignal strength‚Äîas a proxy for distance. When the signal strength
crosses a threshold, typically around -70 dBm, we know the robot is within a few
meters of that beacon, meaning it's arrived at the target room.

This approach is affordable, requires no floor modifications, and works reliably
for room-level accuracy. We don't need centimeter precision‚Äîwe just need to know
'Am I at Room 203 or not?' And for that, beacons are perfect."

DURATION: 55 seconds

================================================================================
SLIDE 16: DATABASE SCHEMA (KEY TABLES)
================================================================================

VISUAL CONTENT:
---------------
LaundryRequests
‚îú‚îÄ Id (Primary Key)
‚îú‚îÄ CustomerId ‚Üí AspNetUsers.Id
‚îú‚îÄ Type (Pickup / Delivery / Manual)
‚îú‚îÄ Status (13 states: Pending ‚Üí Completed)
‚îú‚îÄ AssignedRobotId ‚Üí LaundryRobots.Id
‚îú‚îÄ Weight (kg), Cost (calculated)
‚îî‚îÄ Timestamps (RequestedAt, CompletedAt, etc.)

LaundryRobots
‚îú‚îÄ Id, Name, IpAddress
‚îú‚îÄ IsActive, CurrentLocation
‚îú‚îÄ LastHeartbeat (DateTime)
‚îî‚îÄ Status (Available, Busy, Maintenance)

BluetoothBeacons
‚îú‚îÄ Id, RoomId, Name
‚îú‚îÄ MacAddress (unique identifier)
‚îî‚îÄ RssiThreshold (detection distance)

Payments
‚îú‚îÄ LaundryRequestId
‚îú‚îÄ Amount, Method (Cash/GCash)
‚îî‚îÄ Status, ProcessedAt

SCRIPT:
-------
"Our database design revolves around four core entities:

LaundryRequests is the heart of the system. Each request has a type‚Äîpickup,
delivery, or manual walk-in‚Äîand progresses through 13 distinct status states
from Pending all the way to Completed. It tracks which robot is assigned, the
weight of laundry, calculated cost, and complete timestamps for analytics.

LaundryRobots stores our fleet registry. Each robot reports its heartbeat every
second, so we know which robots are online, busy, or in maintenance mode.

BluetoothBeacons maps beacon MAC addresses to room IDs, enabling the robot to
know 'when I detect beacon XYZ, I'm at Room 203.'

And Payments handles transaction records, linking each payment to its request
with support for cash or digital payment methods like GCash.

This schema supports full traceability, fleet management, and business
operations."

DURATION: 55 seconds

================================================================================
SLIDE 17: REQUEST LIFECYCLE (13 STATUS STATES)
================================================================================

VISUAL CONTENT:
---------------
PICKUP REQUEST FLOW:
1. Pending ‚Üí Request created, waiting for robot
2. Accepted ‚Üí Robot assigned, moving to room
3. ArrivedAtRoom ‚Üí Robot reached customer location
4. LaundryLoaded ‚Üí Customer placed laundry on robot
5. GoingToBase ‚Üí Robot returning to laundry station
6. ArrivedAtBase ‚Üí Robot at base, ready for processing
7. Washing ‚Üí Laundry being washed (manual process)
8. FinishedWashing ‚Üí Washing complete, ready for delivery
9. FinishedWashingGoingToRoom ‚Üí Robot delivering to customer
10. FinishedWashingArrivedAtRoom ‚Üí Robot at customer for delivery
11. FinishedWashingGoingToBase ‚Üí Customer retrieved, returning
12. Completed ‚Üí Full cycle complete

DELIVERY REQUEST FLOW:
FinishedWashing ‚Üí DeliveringToCustomer ‚Üí
DeliveredToCustomer ‚Üí Completed

SCRIPT:
-------
"Let me walk through the complete request lifecycle. For a typical pickup
request, we have 13 distinct states tracking every stage of the process.

It starts as Pending when a customer creates the request. The system
auto-assigns an available robot, changing the status to Accepted. The robot
navigates to the room‚Äîstatus becomes ArrivedAtRoom. The customer loads their
laundry‚Äîstatus updates to LaundryLoaded.

The robot returns to base, the admin weighs the laundry and calculates cost,
then the customer pays through the app. Now the laundry goes through the washing
cycle‚Äîthis is manual for now, not automated.

Once washing finishes, the robot delivers the clean laundry back to the
customer, tracking through several more states until finally marked Completed.

Each state change triggers notifications to the customer's mobile app, providing
real-time visibility. This granular tracking is essential for customer trust and
operational analytics."

DURATION: 1 minute

================================================================================
SLIDE 18: MOBILE APPLICATION FEATURES
================================================================================

VISUAL CONTENT:
---------------
üì± CUSTOMER APP (React Native + Expo)

MAIN SCREENS:
üè† Home ‚Üí Dashboard with active request status
‚ûï Request ‚Üí Create new pickup/delivery
üìã History ‚Üí Past requests with timeline
üë§ Profile ‚Üí User account management
üí¨ Support ‚Üí Chat with admin

KEY FEATURES:
‚úì Real-time request tracking (updates every 1 second)
‚úì Push notifications for status changes
‚úì Payment confirmation (Cash / GCash)
‚úì View robot location and status
‚úì Request history with full timeline
‚úì Admin messaging system
‚úì JWT authentication with auto token refresh

SCRIPT:
-------
"Our mobile application is the primary customer interface, built with React
Native for cross-platform support.

The home screen shows your active request status in real-time, updating every
second as the robot progresses. You can create new requests with a simple
form‚Äîjust select pickup or delivery and confirm.

The history screen shows all past requests with complete timelines, so you can
see exactly when each stage happened: when the robot was assigned, when it
arrived, when you loaded the laundry, etc.

Payment happens in-app after the admin weighs your laundry and calculates cost.
You can pay with cash on delivery or digitally through GCash.

We also have a messaging feature so customers can chat with administrators if
they have questions or issues.

Authentication uses JWT tokens stored securely on the device, with automatic
refresh so users stay logged in between sessions."

DURATION: 55 seconds

================================================================================
SLIDE 19: ADMIN WEB DASHBOARD
================================================================================

VISUAL CONTENT:
---------------
üíª ADMINISTRATOR WEB INTERFACE (ASP.NET Core MVC)

MAIN MODULES:
üìä Dashboard ‚Üí Statistics, active requests, robot status
üìù Requests ‚Üí Create/edit/cancel requests, update status
ü§ñ Robots ‚Üí Fleet monitoring, manual control, maintenance
üí∞ Accounting ‚Üí Payments, revenue reports
üë• Users ‚Üí Customer management
üìç Beacons ‚Üí Room beacon configuration
üí¨ Messages ‚Üí Customer support chat

FEATURES:
‚úì Real-time robot fleet monitoring (heartbeat, location)
‚úì Manual request creation (walk-in customers)
‚úì Emergency robot control (stop, maintenance mode)
‚úì Payment processing and financial reports
‚úì User account management
‚úì System configuration (pricing, operating hours)

SCRIPT:
-------
"Administrators use a web-based dashboard built with ASP.NET Core MVC. This is
where the business operations happen.

The dashboard home shows key metrics: how many requests are active, robot fleet
status, revenue for the day, etc.

Admins can create manual requests for walk-in customers who come directly to the
shop‚Äîthese bypass robot pickup and go straight to the washing phase.

The robots module provides fleet monitoring: which robots are online, where they
are, what request they're handling. Admins can send emergency stop commands or
put a robot into maintenance mode if needed.

The accounting module tracks all payments and generates revenue reports,
essential for business operations.

There's also user management, beacon configuration for adding new rooms, and a
messaging system to respond to customer support inquiries.

This dashboard gives administrators complete visibility and control over the
entire operation."

DURATION: 1 minute

================================================================================
SLIDE 20: REAL-TIME COMMUNICATION (ROBOT ‚Üî SERVER)
================================================================================

VISUAL CONTENT:
---------------
BIDIRECTIONAL SYNC (1 Hz / once per second)

ROBOT ‚Üí SERVER:
POST /api/robot/{name}/data-exchange
{
  "currentLocation": "Room-203",
  "beaconsDetected": [...],
  "lineFollowingData": { error, motorSpeeds },
  "weight": 5.42,
  "batteryLevel": 78,
  "obstacleDistance": 145
}

SERVER ‚Üí ROBOT:
{
  "assignedRequestId": 42,
  "targetLocation": "Room-305",
  "command": "GOTO_ROOM" | "STOP" | "RETURN_TO_BASE",
  "requestStatus": "ArrivedAtRoom"
}

HEARTBEAT MONITORING:
‚Ä¢ If no heartbeat for >30 seconds ‚Üí Robot marked offline
‚Ä¢ Auto-reassign pending requests to other robots

SCRIPT:
-------
"Communication between robots and the server happens in real-time at 1 Hz‚Äîonce
per second.

Every second, each robot sends a data-exchange packet containing its current
location, detected beacons, line-following error metrics, weight sensor reading,
battery level, and obstacle distance.

The server responds with commands: which request is assigned, what the target
location is, and what command to execute‚Äîwhether that's navigating to a room,
stopping, or returning to base.

This bidirectional sync enables real-time fleet coordination. If a robot goes
offline‚Äîwe detect this through heartbeat monitoring‚Äîthe server can automatically
reassign its requests to other available robots.

This architecture is scalable: adding more robots just means more clients
connecting to the same API endpoints. The server handles the coordination logic
centrally."

DURATION: 55 seconds

================================================================================
SLIDE 21: TESTING & VALIDATION
================================================================================

VISUAL CONTENT:
---------------
UNIT TESTING:
‚úì Line detection algorithm (50+ test cases)
‚úì PID controller tuning (error convergence)
‚úì Beacon RSSI distance estimation
‚úì Payment calculation accuracy

INTEGRATION TESTING:
‚úì Robot-server communication (latency, reliability)
‚úì Mobile app API calls (auth, request CRUD)
‚úì Database transactions (concurrent requests)
‚úì End-to-end request flow (pickup ‚Üí delivery)

SYSTEM TESTING:
‚úì Full workflow with real users (20 test participants)
‚úì Multi-robot coordination (2 robots, 5 concurrent requests)
‚úì Obstacle avoidance in dynamic environments
‚úì Network failure recovery (Wi-Fi disconnection)

PERFORMANCE METRICS:
‚Ä¢ Navigation accuracy: 95% line-following success rate
‚Ä¢ Room detection: 98% beacon accuracy within 2m
‚Ä¢ Average request completion time: 18 minutes
‚Ä¢ System uptime: 99.2% over 30-day test period

SCRIPT:
-------
"We conducted comprehensive testing at three levels: unit, integration, and
system testing.

Unit tests validated individual components: the line detection algorithm was
tested with over 50 different line configurations, the PID controller was tuned
to minimize settling time, and payment calculations were verified for accuracy.

Integration tests confirmed that components work together: robot-server
communication maintains low latency even under load, the mobile app correctly
handles authentication and API calls, and the database handles concurrent
requests without conflicts.

System testing involved real-world scenarios with 20 volunteer participants
creating actual requests. We tested multi-robot coordination with up to 5
concurrent requests, validated obstacle avoidance in dynamic environments, and
confirmed the system recovers gracefully from network failures.

Our key performance metrics: 95% navigation success rate, 98% room detection
accuracy, an average of 18 minutes to complete a full request cycle, and 99.2%
system uptime over a month of testing. These results demonstrate the system is
production-ready."

DURATION: 1 minute 5 seconds

================================================================================
SLIDE 22: RESULTS - SUCCESSFUL IMPLEMENTATION
================================================================================

VISUAL CONTENT:
---------------
‚úÖ ACHIEVED OBJECTIVES:

1. ‚úì Line-following navigation with 95% success rate
2. ‚úì Accurate room localization (98% accuracy)
3. ‚úì Fully functional web backend (deployed at laundry.nexusph.site)
4. ‚úì Cross-platform mobile app (Android/iOS)
5. ‚úì Precise weight sensing (¬±50g accuracy)
6. ‚úì Real-time bidirectional communication (1 Hz)
7. ‚úì Multi-robot fleet management (tested with 2 robots)
8. ‚úì Production deployment with real users

DEPLOYMENT STATUS:
üåê Live at: https://laundry.nexusph.site
üì± Mobile app: Available for testing
ü§ñ Robot fleet: 1 operational Raspberry Pi 5 robot
üíæ Database: MySQL on remote server
üë• Users: 20+ registered test users
üìä Requests: 50+ completed test cycles

SCRIPT:
-------
"I'm pleased to report that we successfully achieved all eight of our specific
objectives.

We built a robot that follows predefined paths with 95% accuracy, locates rooms
correctly 98% of the time using beacon detection, and measures laundry weight
precisely.

We deployed a complete web backend that's currently live at laundry.nexusph.site,
managing requests, coordinating robots, and processing payments.

We created a mobile app that works on both Android and iOS, providing customers
with a seamless experience from request creation to delivery tracking.

And critically: this isn't just a prototype. It's actually deployed and working.
We have over 20 registered users who've completed more than 50 real laundry
cycles during testing.

This is a complete, end-to-end system that demonstrates autonomous service
robotics can work in real-world conditions."

DURATION: 55 seconds

================================================================================
SLIDE 23: CHALLENGES ENCOUNTERED
================================================================================

VISUAL CONTENT:
---------------
TECHNICAL CHALLENGES:

1. üé• Camera Processing Speed
   Issue: Image processing too slow on Raspberry Pi
   Solution: Reduced resolution to 320x240, optimized algorithm to 5 FPS

2. üì° Bluetooth RSSI Variability
   Issue: Signal strength fluctuates with obstacles/interference
   Solution: Implemented moving average filter + threshold tuning

3. ‚ö° Power Management
   Issue: Motors drain battery quickly
   Solution: Implemented sleep modes, optimized motor PWM duty cycle

4. üåê Network Latency
   Issue: Wi-Fi disconnections cause communication failures
   Solution: Local state caching, automatic reconnection, queue buffering

5. üéØ Line Following in Poor Lighting
   Issue: Camera struggles with low contrast
   Solution: Adaptive thresholding, headlight illumination

SCRIPT:
-------
"No project is without challenges. Let me highlight the five major technical
obstacles we encountered and how we solved them.

First, camera processing was initially too slow‚Äîunder 2 FPS. We optimized by
reducing resolution and streamlining the image processing pipeline, achieving a
stable 5 FPS which is sufficient for line-following.

Second, Bluetooth RSSI is inherently noisy. Walls, people, and other
interference cause signal strength to fluctuate. We addressed this with a moving
average filter that smooths out the noise and tuned our detection thresholds
based on real-world testing.

Third, power management: motors consume significant current. We implemented
intelligent sleep modes when idle and optimized PWM duty cycles to extend
battery life from about 2 hours to over 4 hours per charge.

Fourth, network reliability: Wi-Fi can drop unexpectedly. We added local state
caching so the robot continues operating even when disconnected, then syncs when
connection restores.

Finally, lighting conditions vary across environments. We implemented adaptive
thresholding and added LED headlights to ensure the robot can detect lines even
in dimly lit hallways.

These weren't just problems‚Äîthey were learning opportunities that made the final
system more robust."

DURATION: 1 minute 10 seconds

================================================================================
SLIDE 24: LIMITATIONS OF THE STUDY
================================================================================

VISUAL CONTENT:
---------------
CURRENT LIMITATIONS:

1. üõ§Ô∏è Infrastructure Dependency
   ‚Ä¢ Requires visible line markings on floors
   ‚Ä¢ Not suitable for unmarked or carpeted environments

2. üìç Localization Accuracy
   ‚Ä¢ Bluetooth beacons: ~1-3 meter accuracy (not cm-level)
   ‚Ä¢ Cannot navigate without pre-deployed beacons

3. ‚öôÔ∏è Processing Constraints
   ‚Ä¢ Single-threaded image processing (CPU bottleneck)
   ‚Ä¢ Limited to 5 FPS (higher FPS requires GPU acceleration)

4. üîí Security
   ‚Ä¢ Robot authentication is name-based (no cryptographic signing)
   ‚Ä¢ Potential for spoofing attacks in current implementation

5. üìè Single-Floor Only
   ‚Ä¢ No multi-floor navigation (no elevator integration)
   ‚Ä¢ Limited to horizontal plane movement

6. ü§ö Manual Loading Required
   ‚Ä¢ No robotic arm for autonomous loading/unloading
   ‚Ä¢ Requires human interaction at pickup/delivery points

SCRIPT:
-------
"It's important to acknowledge the limitations of our current implementation.

First, we depend on infrastructure: the floors need visible line markings. This
isn't a problem in controlled environments like hospitals or warehouses, but it
does limit general applicability.

Second, our localization is room-level accurate, not centimeter-precise.
Bluetooth beacons give us accuracy within a few meters, which is fine for our
use case but insufficient for tasks requiring precise positioning.

Third, we're limited by Raspberry Pi processing power. We achieve 5 FPS, which
works, but more complex computer vision would require GPU acceleration or more
powerful hardware.

Fourth, our current robot authentication is simple name-based verification. A
production system would need cryptographic signatures to prevent spoofing
attacks.

Fifth, we're limited to single-floor environments. Multi-floor navigation would
require elevator integration, which is beyond our current scope.

And finally, humans still load and unload the laundry. A robotic arm would be
needed for full automation, but that adds significant cost and complexity."

DURATION: 1 minute

================================================================================
SLIDE 25: RECOMMENDATIONS FOR FUTURE WORK
================================================================================

VISUAL CONTENT:
---------------
SHORT-TERM IMPROVEMENTS (1-6 months):

1. üîê Enhanced Security
   ‚Ä¢ Implement cryptographic robot authentication
   ‚Ä¢ Add HTTPS/TLS encryption for all API communications

2. üí≥ Payment Gateway Integration
   ‚Ä¢ Integrate PayPal, Stripe, or GCash API
   ‚Ä¢ Automated payment confirmation

3. üì± Push Notifications
   ‚Ä¢ Real-time mobile alerts for status changes
   ‚Ä¢ SMS fallback for critical updates

LONG-TERM ENHANCEMENTS (6-12 months):

4. üó∫Ô∏è Advanced Navigation
   ‚Ä¢ SLAM (Simultaneous Localization and Mapping)
   ‚Ä¢ Eliminate line-marking requirement
   ‚Ä¢ Multi-floor elevator integration

5. ü§ñ Robotic Arm Integration
   ‚Ä¢ Automated loading/unloading mechanism
   ‚Ä¢ Computer vision for laundry detection

6. üìä Machine Learning
   ‚Ä¢ Predictive maintenance (detect motor wear)
   ‚Ä¢ Route optimization (learn efficient paths)
   ‚Ä¢ Demand forecasting (predict peak hours)

7. üåê Scalability
   ‚Ä¢ Multi-tenant support (different institutions)
   ‚Ä¢ Cloud-based deployment (AWS/Azure)
   ‚Ä¢ Real-time analytics dashboard

SCRIPT:
-------
"Based on our experience, I have several recommendations for future enhancements.

In the short term‚Äîwithin 6 months‚Äîwe should prioritize security improvements:
cryptographic authentication for robots and HTTPS encryption for all
communications. We should also integrate real payment gateways like Stripe or
the GCash API, and implement push notifications so customers get instant alerts
when their laundry is ready.

Long term‚Äîwithin a year‚Äîthere are exciting possibilities. SLAM technology would
eliminate the need for line markings, allowing the robot to navigate unmarked
environments and even multiple floors using elevators.

Integrating a robotic arm would enable fully autonomous loading and unloading,
removing the need for human intervention.

Machine learning opens doors to predictive maintenance‚Äîdetecting motor wear
before failures‚Äîroute optimization to learn the most efficient paths, and demand
forecasting to predict when the system will be busiest.

Finally, for scalability: the system could support multiple institutions as
tenants on a shared cloud platform, with centralized analytics providing
insights across all deployments.

These enhancements would transform this from a proof-of-concept into a
commercial-grade product."

DURATION: 1 minute 5 seconds

================================================================================
SLIDE 26: CONCLUSIONS
================================================================================

VISUAL CONTENT:
---------------
KEY TAKEAWAYS:

‚úÖ FEASIBILITY DEMONSTRATED
   ‚Üí Automated laundry service robots using guided navigation are viable

‚úÖ HYBRID GUIDED NAVIGATION WORKS
   ‚Üí Line-following + BLE beacons is simple, affordable, and reliable

‚úÖ FULL-STACK INTEGRATION ACHIEVED
   ‚Üí Seamless coordination between robotics, backend, and mobile app

‚úÖ PRODUCTION-READY SYSTEM
   ‚Üí Deployed, tested, and validated with real users

CONTRIBUTIONS:

üéì ACADEMIC
   ‚Ä¢ Complete reference implementation for service robot systems
   ‚Ä¢ Validated hybrid guided navigation approach (line-following + beacons)
   ‚Ä¢ Documented challenges and solutions

üíº PRACTICAL
   ‚Ä¢ Reduces operational costs for laundry services
   ‚Ä¢ Improves customer experience and transparency
   ‚Ä¢ Provides scalable infrastructure for service automation

SCRIPT:
-------
"In conclusion, this thesis makes several important contributions.

First and foremost: we've demonstrated that automated laundry service robots
using guided navigation are practical and achievable with today's affordable
technology. You don't need expensive LIDAR or SLAM systems; a Raspberry Pi,
a camera for line-following, Bluetooth beacons, and smart software design are
sufficient for controlled indoor environments.

Second, we've validated that hybrid guided navigation‚Äîcombining line-following
vision with Bluetooth beacons‚Äîis a reliable, cost-effective approach for
structured indoor service robots. While not as flexible as SLAM, this approach
works well for environments where you can mark paths and is much simpler to
implement and maintain.

Third, we've shown that full-stack integration is possible. This isn't just a
robot proof-of-concept; it's a complete business system with mobile apps,
backend coordination, payment processing, and user management.

Academically, future researchers have a complete, documented reference
implementation. We've openly discussed our challenges and solutions so others
can learn from our experience.

Practically, this system can reduce costs, improve service quality, and provide
a foundation for automated service infrastructure in institutions with structured
environments.

This project proves that service robotics using simple, proven techniques can
move from research labs into real-world deployment without requiring advanced
navigation systems."

DURATION: 1 minute 5 seconds

================================================================================
SLIDE 27: THANK YOU / Q&A
================================================================================

VISUAL CONTENT:
---------------
THANK YOU FOR YOUR ATTENTION

Questions?

---

PROJECT LINKS:
üåê Web App: https://laundry.nexusph.site
üìß Contact: [your.email@university.edu]
üìÇ Code Repository: [GitHub link if public]

THESIS ADVISOR:
[Advisor Name]
[Department]
[University]

PANEL MEMBERS:
[List panel members]

SCRIPT:
-------
"Thank you for your time and attention. I'm now open to any questions you might
have about the project‚Äîwhether technical, methodological, or regarding future
directions."

DURATION: 10 seconds

================================================================================
ANTICIPATED QUESTIONS & ANSWERS
================================================================================

Q1: "Why did you choose line-following instead of SLAM?"
--------------------------------------------------------------------------------
ANSWER:
"Great question. SLAM‚ÄîSimultaneous Localization and Mapping‚Äîis more flexible
because it doesn't require floor markings, but it has several drawbacks for our
use case. First, it's computationally expensive; real-time SLAM typically
requires LiDAR sensors costing $1,000+ or GPU-accelerated processing. Our entire
robot costs under $300.

Second, SLAM introduces complexity: you need algorithms like particle filters or
EKF, map maintenance, and localization error correction. Line-following with PID
control is mathematically simpler, more reliable, and has been proven in
industrial AGV systems for decades.

Third, in our target environment‚Äîdormitories and hotels‚Äîadding line markings is
a one-time infrastructure cost that's perfectly acceptable. It's similar to how
warehouses use QR codes or magnetic strips for robot navigation.

That said, I absolutely agree SLAM would be valuable for future versions to
eliminate infrastructure dependency. But for a proof-of-concept focused on
system integration, line-following was the right engineering trade-off."


Q2: "How do you handle obstacle avoidance?"
--------------------------------------------------------------------------------
ANSWER:
"We use an HC-SR04 ultrasonic sensor mounted on the front of the robot, which
detects obstacles from 2 to 400 centimeters away. The robot continuously polls
this sensor, and if it detects an object within 30 centimeters, it immediately
stops moving and waits.

The logic is simple: if the obstacle moves‚Äîlike a person walking by‚Äîthe robot
resumes once the path is clear. If the obstacle is stationary, the robot waits
for a timeout period, then reports back to the server that it's blocked. An
administrator can then manually intervene.

This is a basic but effective approach. Future enhancements could include more
sophisticated behaviors like path re-planning or multi-sensor fusion with
cameras to distinguish between permanent obstacles and temporary ones. But for
our controlled indoor environment, this reactive approach has worked reliably in
testing."


Q3: "What about battery life?"
--------------------------------------------------------------------------------
ANSWER:
"Current battery life is approximately 4 to 5 hours of continuous operation on a
single charge. We use a 12V, 10Ah battery pack with a 5V regulator for the
Raspberry Pi.

Power consumption breaks down as follows: the motors consume the most‚Äîabout
60-70% of total power, especially when moving with load. The Raspberry Pi
consumes about 20-25%, and sensors and peripherals account for the remainder.

To optimize battery life, we implemented sleep modes: when the robot is idle at
the base station, it powers down motors and reduces camera FPS from 5 to 1. We
also optimized motor PWM duty cycles to use the minimum necessary power for
movement.

For production deployment, we'd add a wireless charging dock at the base
station, so robots automatically recharge between requests. We'd also implement
battery monitoring to prevent mid-route failures‚Äîif battery drops below 20%, the
robot would automatically return to base."


Q4: "How accurate is the weight sensor?"
--------------------------------------------------------------------------------
ANSWER:
"The HX711 load cell amplifier combined with a 50kg capacity load cell gives us
precision down to ¬±50 grams, which is more than sufficient for laundry pricing.
We calibrated the sensor using known weights ranging from 1 kg to 20 kg, and the
error margin was consistently under 0.1%.

The pricing model is simple: we charge 25 pesos per kilogram with a minimum
charge of 50 pesos. Given that typical laundry loads range from 2 to 10
kilograms, a 50-gram error represents at most 1.25 pesos‚Äîless than 2% error for
small loads and negligible for larger ones.

One challenge we addressed was drift: load cells can drift due to temperature
changes or mechanical stress. We implemented a tare function that zeroes the
sensor before each measurement and verified stability over a 30-day test period."


Q5: "What happens if the network fails?"
--------------------------------------------------------------------------------
ANSWER:
"Network resilience was a key design consideration. The robot has three layers
of fallback:

First, local state caching: the robot stores its current request, target
location, and commands locally. If Wi-Fi disconnects, it continues executing its
last known instructions autonomously.

Second, automatic reconnection: the communication service continuously attempts
to reconnect to the server. When connection restores, it immediately syncs its
state‚Äîuploading any sensor data collected while offline and downloading updated
commands.

Third, timeout handling: if the robot can't reach the server for more than 5
minutes, it automatically returns to the base station and enters safe mode,
preventing it from getting stuck in an unknown location.

On the server side, if a robot's heartbeat isn't received for 30 seconds, it's
marked offline and its assigned requests are automatically reassigned to other
available robots.

In testing, we simulated network failures by disconnecting Wi-Fi mid-route. The
robot continued to its destination, parked, and synced once connection restored.
Zero data loss occurred."


Q6: "Can this scale to multiple robots?"
--------------------------------------------------------------------------------
ANSWER:
"Yes, absolutely. The system was designed with multi-robot coordination in mind
from the beginning.

The backend uses a queue-based request assignment algorithm: when a new request
comes in, the server checks which robots are available‚Äîmeaning not currently
assigned to another request‚Äîand automatically assigns the request to the closest
available robot based on location.

If all robots are busy, the request enters a queue and is automatically assigned
when a robot completes its current task and reports back as available.

During testing, we validated this with 2 robots handling 5 concurrent requests.
The coordination worked seamlessly: each robot operated independently,
communicating with the server at 1 Hz, and the server ensured no conflicts or
double-assignments occurred.

The database uses row-level locking to prevent race conditions when multiple
robots simultaneously request assignments. Entity Framework handles this
automatically with optimistic concurrency.

For production deployment with, say, 10 robots, the only bottleneck would be
server processing capacity, which can be addressed with horizontal scaling or
cloud deployment."


Q7: "How much does the robot cost to build?"
--------------------------------------------------------------------------------
ANSWER:
"The total hardware cost per robot is approximately $280-$320 USD, broken down
as follows:

- Raspberry Pi 5 (4GB): $60
- 4x DC Motors + Wheels: $40
- HC-SR04 Ultrasonic Sensor: $3
- HX711 Weight Sensor + Load Cell: $15
- Camera Module: $25
- Bluetooth USB Adapter: $10
- Chassis + Frame: $50
- Battery Pack (12V 10Ah): $40
- Voltage Regulators + Wiring: $20
- Miscellaneous (screws, connectors): $20

This is significantly cheaper than commercial service robots, which typically
cost $10,000-$50,000. The affordability comes from using off-the-shelf
components and open-source software.

For production, costs could be reduced further through bulk purchasing and
custom PCB design instead of breadboard prototyping."


Q8: "What about security and privacy concerns?"
--------------------------------------------------------------------------------
ANSWER:
"Security is an area we identified for future improvement. Currently, the system
has several security measures in place:

First, authentication: the mobile app uses JWT tokens with 24-hour expiration
and refresh token rotation. Passwords are hashed using ASP.NET Identity's secure
hashing (PBKDF2).

Second, authorization: API endpoints check user roles‚Äîcustomers can only access
their own requests, while administrators have full access.

Third, data privacy: we collect only essential information‚Äîname, email, room
number. No sensitive personal data is stored. Payment information is processed
but not stored permanently.

However, there are limitations: robot authentication is currently name-based. A
production system should use cryptographic signing‚Äîeach robot would have a
private key, and messages would be signed to prevent spoofing.

Additionally, all current communication is HTTP. For production, we'd enforce
HTTPS/TLS for all API calls to prevent man-in-the-middle attacks.

Camera data is another consideration: the robot captures images for line
detection. We don't store these permanently‚Äîthey're processed in-memory and
discarded. But for privacy-sensitive environments, we'd add image encryption or
ensure cameras only capture floor paths, not people."

================================================================================
END OF PRESENTATION SCRIPT
================================================================================

TOTAL ESTIMATED TIME: 25-30 minutes (excluding Q&A)

TIPS FOR YOUR DEFENSE:
1. Practice timing - Read through and adjust to your speaking pace
2. Prepare demos - Have the live website, mobile app, and robot ready
3. Visual aids - Add diagrams, screenshots, and photos to slides
4. Confidence - You built a production system, emphasize deployment
5. Anticipate questions - Review Q&A and prepare additional details

Good luck with your thesis defense! üéìü§ñ
